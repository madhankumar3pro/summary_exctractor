{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 7335487,
          "sourceType": "datasetVersion",
          "datasetId": 4258479
        },
        {
          "sourceId": 7347356,
          "sourceType": "datasetVersion",
          "datasetId": 4266431
        },
        {
          "sourceId": 7348964,
          "sourceType": "datasetVersion",
          "datasetId": 4267486
        }
      ],
      "dockerImageVersionId": 30627,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pypdf langchain transformers tiktoken -q"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "lUuA53onGa-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain import PromptTemplate,  LLMChain\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain import HuggingFacePipeline\n",
        "import transformers\n",
        "import torch\n",
        "import time"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-06T09:31:31.983609Z",
          "iopub.execute_input": "2024-01-06T09:31:31.983975Z",
          "iopub.status.idle": "2024-01-06T09:31:53.254889Z",
          "shell.execute_reply.started": "2024-01-06T09:31:31.983925Z",
          "shell.execute_reply": "2024-01-06T09:31:53.253779Z"
        },
        "trusted": true,
        "id": "2vCTbH4WGa-L",
        "outputId": "89edf23a-f20e-4829-9d6a-3d9946dcd9e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"/kaggle/input/harry-porter/harry-potter-book-collection-1-4.pdf\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-06T10:00:03.052043Z",
          "iopub.execute_input": "2024-01-06T10:00:03.052445Z",
          "iopub.status.idle": "2024-01-06T10:00:03.064165Z",
          "shell.execute_reply.started": "2024-01-06T10:00:03.052417Z",
          "shell.execute_reply": "2024-01-06T10:00:03.063223Z"
        },
        "trusted": true,
        "id": "BVtKASBFGa-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"tiiuae/falcon-7b-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "#     load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "_JElcr95Ga-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline1 = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    max_length=800,\n",
        "    max_new_tokens=70,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    pad_token_id=50256,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "llm1 = HuggingFacePipeline(pipeline = pipeline1, model_kwargs = {'temperature':0})"
      ],
      "metadata": {
        "trusted": true,
        "id": "IuNmHDGkGa-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline2 = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    max_length=3900,\n",
        "    max_new_tokens=300,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    pad_token_id=50256,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "llm2 = HuggingFacePipeline(pipeline = pipeline2, model_kwargs = {'temperature':0})"
      ],
      "metadata": {
        "trusted": true,
        "id": "UDrMhTduGa-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template1 = \"\"\"Generate a meaningful summary of the following text delimited by triple backquotes.\n",
        "              Return your response in short and crisp meaningful summary.\n",
        "              ```{text}```\n",
        "              Summary:\n",
        "           \"\"\"\n",
        "template2 = \"\"\"Generate a meaningful summary of the following text delimited by triple backquotes.\n",
        "              Return your response in short and crisp meaningful summary.\n",
        "              ```{text}```\n",
        "              Summary:\n",
        "           \"\"\"\n",
        "prompt1 = PromptTemplate(template=template1, input_variables=[\"text\"])\n",
        "llm_chain1 = LLMChain(prompt=prompt1, llm=llm1)\n",
        "\n",
        "prompt2 = PromptTemplate(template=template2, input_variables=[\"text\"])\n",
        "llm_chain2 = LLMChain(prompt=prompt2, llm=llm2)"
      ],
      "metadata": {
        "trusted": true,
        "id": "OXasCXwmGa-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_story(story):\n",
        "    article = []\n",
        "    model_generated = []\n",
        "    try:\n",
        "        pages = story.load_and_split()\n",
        "        short_summary = ''\n",
        "        if len(pages)>50:\n",
        "            for i in range(1,len(pages),5):\n",
        "                text = pages[i].page_content\n",
        "                article.append(text)\n",
        "                if len(text.split(' '))>20:\n",
        "                    response = llm_chain1.run(text)\n",
        "                    model_generated.append(' '.join(response.split(' ')))\n",
        "                    short_summary += \" \" + ' '.join(response.split(' '))\n",
        "                else:\n",
        "                    continue\n",
        "        else:\n",
        "            for i in range(1,len(pages)):\n",
        "                text = pages[i].page_content\n",
        "                article.append(text)\n",
        "                if len(text.split(' '))>20:\n",
        "                    response = llm_chain1.run(text)\n",
        "                    model_generated.append(' '.join(response.split(' ')))\n",
        "                    short_summary += \" \" + ' '.join(response.split(' '))\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "    except:\n",
        "        remaining_text = short_summary\n",
        "\n",
        "    final_summary = ''\n",
        "    remaining_text = short_summary\n",
        "    try:\n",
        "        if len(remaining_text.split(' ')) <= 3500 and len(remaining_text.split(' ')) > 1000:\n",
        "            remaining_text = llm_chain2.run(remaining_text)\n",
        "        elif len(remaining_text.split(' ')) <= 1000:\n",
        "            response_chunk = llm_chain1.run(remaining_text)\n",
        "            remaining_text = ' '.join(response_chunk.split(' '))\n",
        "        else:\n",
        "            while remaining_text:\n",
        "                text_splitter = TokenTextSplitter(chunk_size=800, chunk_overlap=0)\n",
        "                texts = text_splitter.split_text(remaining_text)\n",
        "                final_summary = ''\n",
        "                for i in range(len(texts)):\n",
        "                    text_chunk = texts[i]\n",
        "                    response_chunk = llm_chain1.run(text_chunk)\n",
        "                    final_summary = final_summary+ \". \" + ' '.join(response_chunk.split(' '))\n",
        "                if len(final_summary.split(' ')) <= 3500 and len(final_summary.split(' ')) > 1000:\n",
        "                    remaining_text = llm_chain2.run(final_summary)\n",
        "                    break\n",
        "                elif len(final_summary.split(' ')) > 3500:\n",
        "                    remaining_text = final_summary\n",
        "                    continue\n",
        "                else:\n",
        "                    remaining_text = final_summary\n",
        "                    break\n",
        "    except:\n",
        "        story_summary = ' '.join(remaining_text.split(' '))\n",
        "        return [story_summary,article,model_generated]\n",
        "    story_summary = ' '.join(remaining_text.split(' '))\n",
        "    return [story_summary,article,model_generated]"
      ],
      "metadata": {
        "trusted": true,
        "id": "A-FyTINuGa-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "data_frame = summarize_story(loader)"
      ],
      "metadata": {
        "trusted": true,
        "id": "XxSKesnZGa-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model evalution**"
      ],
      "metadata": {
        "id": "nCOw4CyFGa-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install rouge\n",
        "# !pip install bert_score\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from rouge import Rouge\n",
        "import pandas as pd\n",
        "from bert_score import score"
      ],
      "metadata": {
        "trusted": true,
        "id": "be40kfNhGa-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lio=[data_frame[1][0:25]]\n",
        "lio1 =[data_frame[2][0:25]]\n",
        "gh1 = {'article':lio[0] ,'model_generated':lio1[0] ,'gpt-3.5': gpt_summary1}\n",
        "data1 = pd.DataFrame(gh1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "qXFD6pVDGa-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_summaries = data2['model_generated'].tolist()\n",
        "reference_summaries = data2['gpt-3.5'].tolist()\n",
        "\n",
        "# Calculate ROUGE scores for the selected samples\n",
        "rouge_scores = rouge.get_scores(generated_summaries, reference_summaries, avg=True)\n",
        "rouge = Rouge()\n",
        "print(\"ROUGE Scores:\", rouge_scores)"
      ],
      "metadata": {
        "trusted": true,
        "id": "tPVOObpnGa-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score = corpus_bleu(reference_summaries, generated_summaries)\n",
        "print(\"BLEU Score for 25 Summaries:\", bleu_score)"
      ],
      "metadata": {
        "trusted": true,
        "id": "JydysOedGa-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P, R, F1 = score(generated_summaries, reference_summaries, lang=\"en\", verbose=True)\n",
        "print(\"BERT Precision:\", P.mean().item())\n",
        "print(\"BERT Recall:\", R.mean().item())\n",
        "print(\"BERT F1 Score:\", F1.mean().item())"
      ],
      "metadata": {
        "trusted": true,
        "id": "MmsKjKmtGa-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "****Mistral 7b****"
      ],
      "metadata": {
        "id": "yaApt5aSGa-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mistral_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "mistral_tokenizer = AutoTokenizer.from_pretrained(mistral_model_id)\n",
        "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
        "    mistral_model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "#     load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "5wuMyruVGa-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mistral_pipeline1 = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=mistral_model,\n",
        "    tokenizer=mistral_tokenizer,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    max_length=750,\n",
        "    max_new_tokens=70,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    pad_token_id=50256,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=mistral_tokenizer.eos_token_id\n",
        ")\n",
        "mistral_llm1 = HuggingFacePipeline(pipeline = mistral_pipeline1, model_kwargs = {'temperature':0})"
      ],
      "metadata": {
        "trusted": true,
        "id": "TT6GxDtiGa-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mistral_pipeline2 = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=mistral_model,\n",
        "    tokenizer=mistral_tokenizer,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    max_length=3900,\n",
        "    max_new_tokens=290,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    pad_token_id=50256,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=mistral_tokenizer.eos_token_id\n",
        ")\n",
        "mistral_llm2 = HuggingFacePipeline(pipeline = mistral_pipeline2, model_kwargs = {'temperature':0})"
      ],
      "metadata": {
        "trusted": true,
        "id": "3v-3qSVVGa-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mistral_template1 = \"\"\"Generate a meaningful summary of the following text delimited by triple backquotes.\n",
        "              Return your response in short and crisp meaningful summary.\n",
        "              ```{text}```\n",
        "           \"\"\"\n",
        "mistral_template2 = \"\"\"Generate a meaningful summary of the following text delimited by triple backquotes.\n",
        "              Return your response in short and crisp meaningful summary.\n",
        "              ```{text}```\n",
        "           \"\"\"\n",
        "mistral_prompt1 = PromptTemplate(template=mistral_template1, input_variables=[\"text\"])\n",
        "mistral_llm_chain1 = LLMChain(prompt=mistral_prompt1, llm=mistral_llm1)\n",
        "\n",
        "mistral_prompt2 = PromptTemplate(template=mistral_template2, input_variables=[\"text\"])\n",
        "mistral_llm_chain2 = LLMChain(prompt=mistral_prompt2, llm=mistral_llm2)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Tmnizyj9Ga-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mistral_summarize_story(story):\n",
        "    article = []\n",
        "    model_generated = []\n",
        "    try:\n",
        "        pages = story.load_and_split()\n",
        "        short_summary = ''\n",
        "        if len(pages)>50:\n",
        "            for i in range(1,len(pages),5):\n",
        "                print(i)\n",
        "                text = pages[i].page_content\n",
        "                article.append(text)\n",
        "                if len(text.split(' '))>20:\n",
        "                    response = mistral_llm_chain1.run(text)\n",
        "                    model_generated.append(' '.join(response.split(' ')))\n",
        "                    short_summary += \" \" + ' '.join(response.split(' '))\n",
        "                else:\n",
        "                    continue\n",
        "        else:\n",
        "            for i in range(1,len(pages)):\n",
        "                text = pages[i].page_content\n",
        "                article.append(text)\n",
        "                if len(text.split(' '))>20:\n",
        "                    response = mistral_llm_chain1.run(text)\n",
        "                    model_generated.append(' '.join(response.split(' ')))\n",
        "                    short_summary += \" \" + ' '.join(response.split(' '))\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "    except:\n",
        "        remaining_text = short_summary\n",
        "\n",
        "    final_summary = ''\n",
        "    remaining_text = short_summary\n",
        "    try:\n",
        "        if len(remaining_text.split(' ')) <= 3500 and len(remaining_text.split(' ')) > 1000:\n",
        "            remaining_text = mistral_llm_chain2.run(remaining_text)\n",
        "        elif len(remaining_text.split(' ')) <= 1000:\n",
        "            response_chunk = mistral_llm_chain1.run(remaining_text)\n",
        "            remaining_text = ' '.join(response_chunk.split(' '))\n",
        "        else:\n",
        "            while remaining_text:\n",
        "                text_splitter = TokenTextSplitter(chunk_size=800, chunk_overlap=0)\n",
        "                texts = text_splitter.split_text(remaining_text)\n",
        "                final_summary = ''\n",
        "                for i in range(len(texts)):\n",
        "                    text_chunk = texts[i]\n",
        "                    response_chunk = mistral_llm_chain1.run(text_chunk)\n",
        "                    final_summary = final_summary+ \". \" + ' '.join(response_chunk.split(' '))\n",
        "                if len(final_summary.split(' ')) <= 3500 and len(final_summary.split(' ')) > 1000:\n",
        "                    remaining_text = mistral_llm_chain2.run(final_summary)\n",
        "                    break\n",
        "                elif len(final_summary.split(' ')) > 3500:\n",
        "                    remaining_text = final_summary\n",
        "                    continue\n",
        "                else:\n",
        "                    remaining_text = final_summary\n",
        "                    break\n",
        "    except:\n",
        "        story_summary = ' '.join(remaining_text.split(' '))\n",
        "        return [story_summary,model_generated]\n",
        "    story_summary = ' '.join(remaining_text.split(' '))\n",
        "    return [story_summary,model_generated]"
      ],
      "metadata": {
        "trusted": true,
        "id": "nCC5Z9PsGa-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "mistral_data_frame = mistral_summarize_story(loader)"
      ],
      "metadata": {
        "trusted": true,
        "id": "eNsssD_fGa-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model evalution of mistral 7b model**"
      ],
      "metadata": {
        "id": "v6lew35eGa-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = pd.read_csv('/kaggle/input/sample/model_generated.csv')\n",
        "data2['mistral_7b'] = mistral_data_frame[1][0:25]"
      ],
      "metadata": {
        "trusted": true,
        "id": "GdqAU4r5Ga-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_summaries = data2['mistral_7b'].tolist()\n",
        "reference_summaries = data2['gpt-3.5'].tolist()\n",
        "\n",
        "# Calculate ROUGE scores for the selected samples\n",
        "rouge = Rouge()\n",
        "rouge_scores = rouge.get_scores(generated_summaries, reference_summaries, avg=True)\n",
        "print(\"ROUGE Scores:\", rouge_scores)"
      ],
      "metadata": {
        "trusted": true,
        "id": "lgg5ZBkeGa-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score = corpus_bleu(reference_summaries, generated_summaries)\n",
        "print(\"BLEU Score for 25 Summaries:\", bleu_score)"
      ],
      "metadata": {
        "trusted": true,
        "id": "y8I0aYBRGa-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P, R, F1 = score(generated_summaries, reference_summaries, lang=\"en\", verbose=True)\n",
        "print(\"BERT Precision:\", P.mean().item())\n",
        "print(\"BERT Recall:\", R.mean().item())\n",
        "print(\"BERT F1 Score:\", F1.mean().item())"
      ],
      "metadata": {
        "trusted": true,
        "id": "TA6KeWVVGa-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gemini pro**"
      ],
      "metadata": {
        "id": "JyVNm4n8Ga-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "Gemini_pro_API_Key = 'API_KEY'\n",
        "genai.configure(api_key=Gemini_pro_API_Key)\n",
        "gemini_model = genai.GenerativeModel('gemini-pro')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-06T12:02:53.657472Z",
          "iopub.execute_input": "2024-01-06T12:02:53.658290Z",
          "iopub.status.idle": "2024-01-06T12:02:53.663116Z",
          "shell.execute_reply.started": "2024-01-06T12:02:53.658259Z",
          "shell.execute_reply": "2024-01-06T12:02:53.662123Z"
        },
        "trusted": true,
        "id": "UlLbze2rGa-w"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-06T12:02:55.578225Z",
          "iopub.execute_input": "2024-01-06T12:02:55.578622Z",
          "iopub.status.idle": "2024-01-06T12:02:55.583140Z",
          "shell.execute_reply.started": "2024-01-06T12:02:55.578589Z",
          "shell.execute_reply": "2024-01-06T12:02:55.581981Z"
        },
        "trusted": true,
        "id": "L8MpIAzfGa-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = f\"\"\"\n",
        "    Generate a meaningful summary of the following text delimited by triple backquotes.\n",
        "    Return your response in short and crisp meaningful summary consist of atleast 500.\n",
        "\n",
        "    ```{text}```\n",
        "    \"\"\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-06T12:02:56.047622Z",
          "iopub.execute_input": "2024-01-06T12:02:56.048430Z",
          "iopub.status.idle": "2024-01-06T12:02:56.052897Z",
          "shell.execute_reply.started": "2024-01-06T12:02:56.048394Z",
          "shell.execute_reply": "2024-01-06T12:02:56.051854Z"
        },
        "trusted": true,
        "id": "3XCAPlF0Ga-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyper_parameter = genai.types.GenerationConfig(\n",
        "                                  candidate_count=1,\n",
        "                                  max_output_tokens=500,\n",
        "                                  top_p = 0.7,\n",
        "                                  top_k = 4,\n",
        "                                  temperature=0.2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-06T12:02:58.933919Z",
          "iopub.execute_input": "2024-01-06T12:02:58.934326Z",
          "iopub.status.idle": "2024-01-06T12:02:58.939561Z",
          "shell.execute_reply.started": "2024-01-06T12:02:58.934294Z",
          "shell.execute_reply": "2024-01-06T12:02:58.938516Z"
        },
        "trusted": true,
        "id": "Sn0d8jk6Ga-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gemini_pro_summary(story,prompt_template,hyper_parameter):\n",
        "    pages = story.load_and_split()\n",
        "    short_pages = ''\n",
        "    for i in range(0,len(pages)):\n",
        "        text = pages[i].page_content\n",
        "        short_pages = short_pages +' '+text\n",
        "    if len(short_pages.split(' ')) < 28000:\n",
        "        text3 = short_pages\n",
        "        prompt1 = prompt_template.format(text=text3)\n",
        "        response = gemini_model.generate_content(prompt1,generation_config=hyper_parameter)\n",
        "        return response.text\n",
        "    else:\n",
        "        while short_pages:\n",
        "            text_splitter = TokenTextSplitter(chunk_size=20000, chunk_overlap=0)\n",
        "            texts = text_splitter.split_text(short_pages)\n",
        "            final_summary = ''\n",
        "            for i in range(1,len(texts)):\n",
        "                concatenated_text = texts[i]\n",
        "                prompt2 = prompt_template.format(text=concatenated_text)\n",
        "                response = gemini_model.generate_content(prompt2,generation_config=hyper_parameter)\n",
        "                print(response.text)\n",
        "                final_summary = final_summary+ \". \" + str(response.parts)\n",
        "\n",
        "            if len(final_summary.split(' ')) > 28000:\n",
        "                short_pages = final_summary\n",
        "                count = count + 1\n",
        "                continue\n",
        "            else:\n",
        "                text2 = final_summary\n",
        "                prompt3 = prompt_template.format(text=text2)\n",
        "                response = gemini_model.generate_content(prompt3,generation_config=hyper_parameter)\n",
        "                break\n",
        "\n",
        "    return response.text\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-06T12:05:33.839261Z",
          "iopub.execute_input": "2024-01-06T12:05:33.839644Z",
          "iopub.status.idle": "2024-01-06T12:05:33.850470Z",
          "shell.execute_reply.started": "2024-01-06T12:05:33.839612Z",
          "shell.execute_reply": "2024-01-06T12:05:33.849382Z"
        },
        "trusted": true,
        "id": "xwaT2TNVGa-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "gemini_pro_summary(loader,prompt_template,hyper_parameter)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-06T12:05:34.945111Z",
          "iopub.execute_input": "2024-01-06T12:05:34.945502Z",
          "iopub.status.idle": "2024-01-06T12:06:18.298610Z",
          "shell.execute_reply.started": "2024-01-06T12:05:34.945463Z",
          "shell.execute_reply": "2024-01-06T12:06:18.297606Z"
        },
        "trusted": true,
        "id": "u6vyrqhGGa-1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gemini pro model evalution**"
      ],
      "metadata": {
        "id": "US6fl-EKGa-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pages = loader.load_and_split()\n",
        "gemini_output1 = []\n",
        "for i in range(1,10):\n",
        "    text = pages[i].page_content\n",
        "    response = gemini_model.generate_content(prompt_template,generation_config=hyper_parameter)\n",
        "    gemini_output1.append(response.text)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-06T10:20:03.694215Z",
          "iopub.execute_input": "2024-01-06T10:20:03.694991Z",
          "iopub.status.idle": "2024-01-06T10:21:13.856046Z",
          "shell.execute_reply.started": "2024-01-06T10:20:03.694951Z",
          "shell.execute_reply": "2024-01-06T10:21:13.855157Z"
        },
        "trusted": true,
        "id": "b80VZEyHGa-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = pd.read_csv('/kaggle/input/sample/model_generated.csv')\n",
        "data2['gemini_pro_advance'] = gemini_output1[0:25]"
      ],
      "metadata": {
        "trusted": true,
        "id": "ewCsupA8Ga-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_summaries = data2['gemini_pro_advance'].tolist()\n",
        "reference_summaries = data2['gpt-3.5'].tolist()\n",
        "# Calculate ROUGE scores for the selected samples\n",
        "rouge = Rouge()\n",
        "rouge_scores = rouge.get_scores(generated_summaries, reference_summaries, avg=True)\n",
        "print(\"ROUGE Scores:\", rouge_scores)"
      ],
      "metadata": {
        "trusted": true,
        "id": "2CoOwAmzGa-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score = corpus_bleu(reference_summaries, generated_summaries)\n",
        "print(\"BLEU Score for 25 Summaries:\", bleu_score)"
      ],
      "metadata": {
        "trusted": true,
        "id": "7w7FJlsfGa-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P, R, F1 = score(generated_summaries, reference_summaries, lang=\"en\", verbose=True)\n",
        "print(\"BERT Precision:\", P.mean().item())\n",
        "print(\"BERT Recall:\", R.mean().item())\n",
        "print(\"BERT F1 Score:\", F1.mean().item())"
      ],
      "metadata": {
        "trusted": true,
        "id": "of_jdD7BGa_X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}